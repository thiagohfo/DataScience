{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Estatística Básica e Análise Exploratória\n",
    "\n",
    "Nesse notebook, nós veremos uma parte da estatística básica já aplicada a uma análise exploratória. Aqui, o objetivo é ver a descrição dos dados e como eles podem nos gerar insights. É claro que essa parte é guiada conforme os dados que nós temos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando as bibliotecas\n",
    "\n",
    "Somente importamos as bibliotecas essenciais para este notebook.\n",
    "\n",
    "Link do SKLearn: https://scikit-learn.org/stable/modules/classes.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configurações básicas para a leitura de arquivos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Mostrar todas as colunas\n",
    "pd.set_option('display.max_rows', 30)  # Mostrar apenas 10 linhas\n",
    "\n",
    "df = pd.read_csv('AP_processed.csv', sep=';')\n",
    "\n",
    "print(\"Tamanho da base: {}\".format(len(df)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizando como está a tabela\n",
    "\n",
    "Particularmente, essa propriedade nos traz quantas features nós temos e quantos rows (entrada de dados)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizando tipos dados que nós temos\n",
    "\n",
    "Essa propriedade retorna o tipo para cada feature, podendo ser: inteiro, string, object e entre outros. Essa informação é importante para sabermos que operações podemos executar conforme cada feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resumo estatístico dos dados\n",
    "\n",
    "Esse método traz todo um resumo estatístico do conjunto de dados. As informações trazidas são:\n",
    "\n",
    "## Estimativas de localização\n",
    "Variáveis com dados de medição ou contagem podem ter milhares de valores diferentes. Um passo fundamental na exploração de seus dados é definir um “valor típico” para cada característica (variável): uma estimativa de onde a maioria dos dados está localizada (ou seja, sua tendência central).\n",
    "\n",
    "- Média: A soma de todos os valores, dividida pelo número de valores\n",
    "- Média ponderada: A soma de todos os valores, multiplicada por um peso e dividida pela soma dos pesos\n",
    "- Mediana: O valor que ocupa a posição central dos dados. Sinônimo: 50° percentil\n",
    "- Mediana ponderada: Valor cuja posição está no centro da soma dos pesos, estando metade da soma antes e metade depois desse dado.\n",
    "- Média aparada: A média de todos os valores depois da exclusão de um número fixo de valores extremos.\n",
    "- Outlier: Um valor de dados que é muito diferente da maioria dos dados.\n",
    "\n",
    "## Estimativas de Variabilidade\n",
    "A localização é apenas uma dimensão na sumarização de uma característica. Uma segunda dimensão, variabilidade, também chamada de dispersão, mede se os valores de dados estão compactados ou espalhados. A variabilidade fica no centro da estatística: medindo, reduzindo, distinguindo variabilidade aleatória de real, identificando as diversas fontes de variabilidade real e tomando decisões em sua presença.\n",
    "\n",
    "- Desvios: A diferença entre os valores observados e a estimativa de localização. Também conhecido como erros ou resíduos.\n",
    "- Variância: A soma dos quadrados dos desvios da média, divididos por n – 1, em que n é o número de valores de dados. Também conhecido como erro médio quadrático.\n",
    "- Desvio-padrão: A raiz quadrada da variância. Norma Euclidiana.\n",
    "- Desvio absoluto médio; A média do valor absoluto dos desvios da média. Norma Manhattan.\n",
    "- Amplitude: A diferença entre o maior e o menor valor no conjunto de dados.\n",
    "- Estatísticas ordinais: Métricas baseadas nos valores de dados classificados do menor ao maior.\n",
    "- Percentil: Valor tal que P por cento dos valores assumam esse valor ou menos, e (100 – P) por cento assumam esse valor ou mais.\n",
    "- Amplitude interquartílica: A diferença entre o 75° percentil e o 25° percentil.\n",
    "\n",
    "## Dados binários ou categóricos\n",
    "\n",
    "São aqueles dados onde não possuem valores numéricos, ou seja, não dá para estimar um valor para uma determinada categoria.\n",
    "- Moda: A categoria, ou valor, de maior ocorrência em um conjunto de dados.\n",
    "- Valor esperado: Quando as categorias podem ser associadas a um valor numérico, isso nos dá um valor médio com base na probabilidade de ocorrência de uma categoria.\n",
    "- Gráficos de barras: A frequência ou proporção de cada categoria representada por barras.\n",
    "- Gráficos de pizza: A frequência ou proporção de cada categoria representada por fatias de uma pizza."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variáveis preditoras e variável alvo/destino\n",
    "\n",
    "## Variáveis preditoras\n",
    "\n",
    "As variáveis preditoras no contexto de machine learning são os dados de entrada ou as variáveis que são mapeadas para a variável de destino/saída por meio de uma relação empírica geralmente determinada por meio dos dados. Nas estatísticas, você se refere a elas como preditoras. Cada conjunto de preditores pode ser chamado como uma observação.\n",
    "\n",
    "## Variável alvo/destino\n",
    "\n",
    "A variável de destino, no contexto de machine learning, é a variável que é ou deveria ser a saída. Por exemplo, pode ser binário 0 ou 1 se você estiver classificando ou pode ser uma variável contínua se estiver fazendo uma regressão. Nas estatísticas, você também se refere a ela como a variável de resposta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = df.groupby('resultadoTeste').size()\n",
    "target.plot.bar()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlação\n",
    "\n",
    "A análise exploratória de dados, em muitos projetos de modelagem (seja em ciências de dados ou em pesquisa), envolve o estudo da correlação entre preditores, e entre preditores e uma variável-alvo. As variáveis X e Y (cada uma com dados medidos) são tidas como positivamente correlacionadas se valores altos de X acompanharem os valores altos de Y, e os valores baixos de X acompanharem os valores baixos de Y. Se os valores altos de X acompanharem os valores baixos de Y, e vice-versa, as variáveis são negativamente correlacionadas.\n",
    "\n",
    "- Coeficiente de correlação: Uma métrica que mede o nível em que as variáveis numéricas estão associadas umas às outras (varia de –1 a +1).\n",
    "- Matriz de correlação: Uma tabela na qual as variáveis são mostradas tanto nas linhas quanto nas colunas, e os valores das células são a correlação entre as variáveis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.corr(method = 'pearson')\n",
    "\n",
    "df_temp = df[['tosse', 'febre', 'garganta', 'dispneia', 'cabeca', 'coriza', 'hipogeusia', 'anosmia', 'fadiga', 'nauseas', 'mialgia', 'diarreia', 'resultadoTeste']]\n",
    "\n",
    "sns.heatmap(df_temp.corr(method='pearson'), xticklabels=df_temp.columns, yticklabels=df_temp.columns, annot=True, fmt='.1f', linewidths=.6, cmap='YlGnBu')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlação de Spearman\n",
    "\n",
    "É um coeficiente de correlação baseado na classificação dos dados. Como trabalha com classificações, em vez de valores, essas estimativas são robustas contra outliers e podem manipular certos tipos de não linearidades."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.corr(method = 'spearman')\n",
    "\n",
    "df_temp = df[['tosse', 'febre', 'garganta', 'dispneia', 'cabeca', 'coriza', 'hipogeusia', 'anosmia', 'fadiga', 'nauseas', 'mialgia', 'diarreia', 'resultadoTeste']]\n",
    "\n",
    "sns.heatmap(df_temp.corr(method='spearman'), xticklabels=df_temp.columns, yticklabels=df_temp.columns, annot=True, fmt='.1f', linewidths=.6, cmap='YlGnBu')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualização dos dados\n",
    "\n",
    "Como estão sendo tratados dados categóricos, binários, a visualização que melhor se adequa é a visualização em gráfico de barras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bar_plot(indexes_t, values_t, ylim=0, xlim=0):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(indexes_t, values_t, width=0.4)\n",
    "\n",
    "    for index, value in enumerate(values_t):\n",
    "        plt.text(index - 0.20, value + 0.02, str(value))\n",
    "\n",
    "    if ylim != 0:\n",
    "        plt.ylim(0, ylim + (ylim * 0.1))\n",
    "\n",
    "    if xlim != 0:\n",
    "        plt.xlim(0, xlim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualização dos sintomas\n",
    "\n",
    "Cada sintoma corresponde a uma barra, onde representa o total de pessoas com aquele determinado sintoma."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "symptoms = ['tosse', 'febre', 'garganta', 'dispneia', 'cabeca', 'coriza', 'hipogeusia', 'anosmia', 'fadiga', 'nauseas', 'mialgia', 'diarreia']\n",
    "\n",
    "for i in symptoms:\n",
    "    values.append((df[i] == 1).sum())\n",
    "\n",
    "bar_plot(symptoms, values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualização das comorbidades\n",
    "\n",
    "Cada comorbidade corresponde a uma barra, onde representa o tal de pessoas com aquela comorbidade."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "conditions = ['cardiacas', 'diabetes', 'respiratorias', 'renais', 'imunologica', 'obesidade', 'imunossupressao']\n",
    "\n",
    "for i in conditions:\n",
    "    values.append((df[i] == 1).sum())\n",
    "\n",
    "bar_plot(conditions, values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Amostragem\n",
    "\n",
    "Um grande equívoco e ocorre até comumente é pensar que a era do big data significa o fim da necessidade de amostragem. Na verdade, a proliferação de dados de qualidade e relevâncias variáveis reforça a necessidade da amostragem como ferramenta para trabalhar eficientemente com uma variedade de dados e para minimizar o viés. Mesmo em um projeto de big data, os modelos preditivos são tipicamente desenvolvidos e conduzidos com amostras\n",
    "<br>\n",
    "<center><img src=\"Images/sample_population.png\" alt=\"MarineGEO circle logo\"/></center>\n",
    "<br>\n",
    "\n",
    "- Amostra: Um subconjunto de um conjunto maior de dados.\n",
    "- População: O conjunto maior de dados, ou a ideia de um conjunto de dados.\n",
    "- N (n): O tamanho da população (amostra).\n",
    "- Amostragem aleatória: Elementos aleatoriamente obtidos para uma amostra.\n",
    "- Amostragem estratificada: Divide a população em estratos e faz amostragens aleatórias em cada estrato.\n",
    "- Amostra aleatória simples: A amostra que resulta de uma amostragem aleatória sem estratificar a população.\n",
    "- Viés de amostragem: Uma amostra que não representa a população.\n",
    "\n",
    "Na era do big data, muitas vezes é surpreendente que menos seja mais. O tempo e o esforço gastos em amostragens aleatórias não apenas reduzem o viés, mas também permitem maior atenção à exploração de dados e qualidade de dados.\n",
    "\n",
    "## Então, quando quantidades massivas de dados são de fato necessárias?\n",
    "\n",
    "O cenário clássico para o valor do big data é quando os dados não são apenas grandes, mas também esparsos. Considere as solicitações de pesquisa recebidas pelo Google, em que as colunas são termos, as linhas são as pesquisas individuais e os valores das células são 0 ou 1, dependendo de a pesquisa conter um termo. O objetivo é determinar o melhor destino previsto para a pesquisa de determinada consulta. Existem mais de 150 mil palavras na língua inglesa, e o Google processa mais de 1 trilhão de pesquisas por ano. Isso resulta em uma matriz enorme, e a grande maioria desses registros é “0”.\n",
    "\n",
    "## Criando um conjunto de amostras\n",
    "\n",
    "### Bootstrap\n",
    "\n",
    "Um jeito fácil e eficaz de estimar a distribuição amostral de uma estatística ou de parâmetros de modelo é extrair amostras adicionais, com reposição, da própria amostra.\n",
    "\n",
    "### Reamostragem\n",
    "\n",
    "Às vezes, o termo reamostragem é usado como sinônimo do termo bootstrapping, em muitas situações. Mais comumente, o termo reamostragem também inclui procedimentos de permutação, em que múltiplas amostras são combinadas e a amostragem pode ser feita sem reposição.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "backup_data = df.copy()  # Backup do dataset para garantir que o processo de balanceamento pega sem alterações\n",
    "\n",
    "for kind in ['Unbalanced', 'Balanced', 'Symptoms_Amount']:\n",
    "    df = backup_data.copy()  # Utiliza o backup para realizar operações em dataset normal\n",
    "\n",
    "    if kind == 'Balanced':\n",
    "        df_positives = df[df['resultadoTeste'] == 1]\n",
    "        df_negatives = df[df['resultadoTeste'] == 0]\n",
    "    elif kind == 'Symptoms_Amount':\n",
    "        df_positives = df_positives[df_positives.iloc[:, 1:13].sum(axis=1) > 3]\n",
    "        df_positives = df_positives[(df_positives['anosmia'] == 1) | (df_positives['hipogeusia'] == 1)]\n",
    "\n",
    "    if kind != 'Unbalanced':\n",
    "        if len(df_positives) > len(df_negatives):\n",
    "            df_majority = df_positives\n",
    "            df_minority = df_negatives\n",
    "        else:\n",
    "            df_majority = df_negatives\n",
    "            df_minority = df_positives\n",
    "\n",
    "        df_size = round((len(df_minority) * 2) * 0.60)\n",
    "        df_reduced = resample(df_majority, replace=True, n_samples=df_size, random_state=123)\n",
    "        df_balanced = pd.concat([df_reduced, df_minority])\n",
    "\n",
    "        df_balanced.to_csv('AP_{}.csv'.format(kind), sep=';', encoding='utf-8', index=False)\n",
    "    else:\n",
    "        df.to_csv('AP_{}.csv'.format(kind), sep=';', encoding='utf-8', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base não balanceada\n",
    "\n",
    "Essa é apenas a base do jeito que veio, passando apenas pelos processo de pré-processamento dos dados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('AP_Unbalanced.csv', sep=';')\n",
    "\n",
    "print(\"Tamanho da base: {}\".format(len(df)))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Base balanceada\n",
    "\n",
    "Esta base já se encontra balanceada, com 40% dos casos para negativos e 60% dos casos para positivo com relação a COVID-19."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('AP_Balanced.csv', sep=';')\n",
    "\n",
    "print(\"Tamanho da base: {}\".format(len(df)))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Base balanceada com customização\n",
    "\n",
    "Nesta base, nós temos o balanceamento de 40/60 como mostrado na base anterior e os 60% dos casos positivos, pegamos registros de pessaos que melhor caracterizassem a doença."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('AP_Symptoms_Amount.csv', sep=';')\n",
    "\n",
    "print(\"Tamanho da base: {}\".format(len(df)))\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import cv2\n",
    "\n",
    "imagem = np.load('tc.npy')\n",
    "#imagem = imagem.astype(np.float64) / imagem.max() # normalize the data to 0 - 1\n",
    "#imagem = 255 * imagem # Now scale by 255\n",
    "#img = imagem.astype(np.uint8)\n",
    "#cv2.imshow(\"Window\", img)\n",
    "\n",
    "for i in range(0, 281):\n",
    "    img = im.fromarray(np.uint8(imagem[:,:,i]))\n",
    "    img.save('CT_Scan/{}.png'.format(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
